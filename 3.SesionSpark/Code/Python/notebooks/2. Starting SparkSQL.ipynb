{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Spark SQL\n",
    "\n",
    "Note that this Cloudera version comes with Spark 1.6. Spark 2.0 is the new and powerful version.\n",
    "\n",
    "Source: some materials from Databricks courses at https://databricks.com/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Context\n",
    "\n",
    "In Spark, communication occurs between a driver and executors.  The driver has Spark jobs that it needs to run and these jobs are split into tasks that are submitted to the executors for completion.  The results from these tasks are delivered back to the driver.\n",
    "\n",
    "In part 1, we saw that normal Python code can be executed via cells. When using Databricks this code gets executed in the Spark driver's Java Virtual Machine (JVM) and not in an executor's JVM, and when using an Jupyter notebook it is executed within the kernel associated with the notebook. Since no Spark functionality is actually being used, no tasks are launched on the executors.\n",
    "\n",
    "In order to use Spark and its DataFrame API we will need to use a `SQLContext`.  When running Spark, you start a new Spark application by creating a [SparkContext](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext). You can then create a [SQLContext](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext) from the `SparkContext`. When the `SparkContext` is created, it asks the master for some cores to use to do work.  The master sets these cores aside just for you; they won't be used for other applications. When using Databricks, both a `SparkContext` and a `SQLContext` are created for you automatically. `sc` is your `SparkContext`, and `sqlContext` is your `SQLContext`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Cluster\n",
    "The diagram shows an example cluster, where the slots allocated for an application are outlined in purple. (Note: We're using the term _slots_ here to indicate threads available to perform parallel work for Spark.\n",
    "Spark documentation often refers to these threads as _cores_, which is a confusing term, as the number of slots available on a particular machine does not necessarily have any relationship to the number of physical CPU\n",
    "cores on that machine.)\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-2a.png\" style=\"height: 800px;float: right\"/>\n",
    "\n",
    "You can view the details of your Spark application in the Spark web UI.  The web UI is accessible in Databricks by going to \"Clusters\" and then clicking on the \"Spark UI\" link for your cluster.  In the web UI, under the \"Jobs\" tab, you can see a list of jobs that have been scheduled or run.  It's likely there isn't any thing interesting here yet because we haven't run any jobs, but we'll return to this page later.\n",
    "\n",
    "At a high level, every Spark application consists of a driver program that launches various parallel operations on executor Java Virtual Machines (JVMs) running either in a cluster or locally on the same machine. In Databricks, \"Databricks Shell\" is the driver program.  When running locally, `pyspark` is the driver program. In all cases, this driver program contains the main loop for the program and creates distributed datasets on the cluster, then applies operations (transformations & actions) to those datasets.\n",
    "Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster. A Spark SQL context object (`sqlContext`) is the main entry point for Spark DataFrame and SQL functionality. A `SQLContext` can be used to create DataFrames, which allows you to direct the operations on your data.\n",
    "\n",
    "Try printing out `sqlContext` to see its type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Part 1: Test Spark functionality **\n",
    "Start creating a new SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.context.SQLContext"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "# Display the type of the Spark sqlContext\n",
    "type(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedContext',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_sc',\n",
       " '_scala_SQLContext',\n",
       " '_ssql_ctx',\n",
       " 'applySchema',\n",
       " 'cacheTable',\n",
       " 'clearCache',\n",
       " 'createDataFrame',\n",
       " 'createExternalTable',\n",
       " 'dropTempTable',\n",
       " 'getConf',\n",
       " 'getOrCreate',\n",
       " 'inferSchema',\n",
       " 'jsonFile',\n",
       " 'jsonRDD',\n",
       " 'load',\n",
       " 'newSession',\n",
       " 'parquetFile',\n",
       " 'range',\n",
       " 'read',\n",
       " 'registerDataFrameAsTable',\n",
       " 'registerFunction',\n",
       " 'setConf',\n",
       " 'sql',\n",
       " 'table',\n",
       " 'tableNames',\n",
       " 'tables',\n",
       " 'udf',\n",
       " 'uncacheTable']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List sqlContext's attributes\n",
    "dir(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SQLContext in module pyspark.sql.context object:\n",
      "\n",
      "class SQLContext(__builtin__.object)\n",
      " |  Main entry point for Spark SQL functionality.\n",
      " |  \n",
      " |  A SQLContext can be used create :class:`DataFrame`, register :class:`DataFrame` as\n",
      " |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      " |  \n",
      " |  :param sparkContext: The :class:`SparkContext` backing this SQLContext.\n",
      " |  :param sqlContext: An optional JVM Scala SQLContext. If set, we do not instantiate a new\n",
      " |      SQLContext in the JVM, instead we make all calls to this object.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sparkContext, sqlContext=None)\n",
      " |      Creates a new SQLContext.\n",
      " |      \n",
      " |      >>> from datetime import datetime\n",
      " |      >>> sqlContext = SQLContext(sc)\n",
      " |      >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      " |      ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      " |      ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      " |      >>> df = allTypes.toDF()\n",
      " |      >>> df.registerTempTable(\"allTypes\")\n",
      " |      >>> sqlContext.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      " |      ...            'from allTypes where b and i > 0').collect()\n",
      " |      [Row(_c0=2, _c1=2.0, _c2=False, _c3=2, _c4=0,             time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      " |      >>> df.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      " |      [(1, u'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      " |  \n",
      " |  applySchema(self, rdd, schema)\n",
      " |      .. note:: Deprecated in 1.3, use :func:`createDataFrame` instead.\n",
      " |  \n",
      " |  cacheTable(self, tableName)\n",
      " |      Caches the specified table in-memory.\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  clearCache(self)\n",
      " |      Removes all cached tables from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  createDataFrame(self, data, schema=None, samplingRatio=None)\n",
      " |      Creates a :class:`DataFrame` from an :class:`RDD` of :class:`tuple`/:class:`list`,\n",
      " |      list or :class:`pandas.DataFrame`.\n",
      " |      \n",
      " |      When ``schema`` is a list of column names, the type of each column\n",
      " |      will be inferred from ``data``.\n",
      " |      \n",
      " |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      " |      from ``data``, which should be an RDD of :class:`Row`,\n",
      " |      or :class:`namedtuple`, or :class:`dict`.\n",
      " |      \n",
      " |      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      " |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      " |      \n",
      " |      :param data: an RDD of :class:`Row`/:class:`tuple`/:class:`list`/:class:`dict`,\n",
      " |          :class:`list`, or :class:`pandas.DataFrame`.\n",
      " |      :param schema: a :class:`StructType` or list of column names. default None.\n",
      " |      :param samplingRatio: the sample ratio of rows used for inferring\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> l = [('Alice', 1)]\n",
      " |      >>> sqlContext.createDataFrame(l).collect()\n",
      " |      [Row(_1=u'Alice', _2=1)]\n",
      " |      >>> sqlContext.createDataFrame(l, ['name', 'age']).collect()\n",
      " |      [Row(name=u'Alice', age=1)]\n",
      " |      \n",
      " |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      " |      >>> sqlContext.createDataFrame(d).collect()\n",
      " |      [Row(age=1, name=u'Alice')]\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(l)\n",
      " |      >>> sqlContext.createDataFrame(rdd).collect()\n",
      " |      [Row(_1=u'Alice', _2=1)]\n",
      " |      >>> df = sqlContext.createDataFrame(rdd, ['name', 'age'])\n",
      " |      >>> df.collect()\n",
      " |      [Row(name=u'Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> Person = Row('name', 'age')\n",
      " |      >>> person = rdd.map(lambda r: Person(*r))\n",
      " |      >>> df2 = sqlContext.createDataFrame(person)\n",
      " |      >>> df2.collect()\n",
      " |      [Row(name=u'Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import *\n",
      " |      >>> schema = StructType([\n",
      " |      ...    StructField(\"name\", StringType(), True),\n",
      " |      ...    StructField(\"age\", IntegerType(), True)])\n",
      " |      >>> df3 = sqlContext.createDataFrame(rdd, schema)\n",
      " |      >>> df3.collect()\n",
      " |      [Row(name=u'Alice', age=1)]\n",
      " |      \n",
      " |      >>> sqlContext.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      " |      [Row(name=u'Alice', age=1)]\n",
      " |      >>> sqlContext.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      " |      [Row(0=1, 1=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  createExternalTable(self, tableName, path=None, source=None, schema=None, **options)\n",
      " |      Creates an external table based on the dataset in a data source.\n",
      " |      \n",
      " |      It returns the DataFrame associated with the external table.\n",
      " |      \n",
      " |      The data source is specified by the ``source`` and a set of ``options``.\n",
      " |      If ``source`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      " |      created external table.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  dropTempTable(self, tableName)\n",
      " |      Remove the temp table from catalog.\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> sqlContext.dropTempTable(\"table1\")\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  getConf(self, key, defaultValue)\n",
      " |      Returns the value of Spark SQL configuration property for the given key.\n",
      " |      \n",
      " |      If the key is not set, returns defaultValue.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  inferSchema(self, rdd, samplingRatio=None)\n",
      " |      .. note:: Deprecated in 1.3, use :func:`createDataFrame` instead.\n",
      " |  \n",
      " |  jsonFile(self, path, schema=None, samplingRatio=1.0)\n",
      " |      Loads a text file storing one JSON object per line as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: Deprecated in 1.4, use :func:`DataFrameReader.json` instead.\n",
      " |      \n",
      " |      >>> sqlContext.jsonFile('python/test_support/sql/people.json').dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |  \n",
      " |  jsonRDD(self, rdd, schema=None, samplingRatio=1.0)\n",
      " |      Loads an RDD storing one JSON object per string as a :class:`DataFrame`.\n",
      " |      \n",
      " |      If the schema is provided, applies the given schema to this JSON dataset.\n",
      " |      Otherwise, it samples the dataset with ratio ``samplingRatio`` to determine the schema.\n",
      " |      \n",
      " |      >>> df1 = sqlContext.jsonRDD(json)\n",
      " |      >>> df1.first()\n",
      " |      Row(field1=1, field2=u'row1', field3=Row(field4=11, field5=None), field6=None)\n",
      " |      \n",
      " |      >>> df2 = sqlContext.jsonRDD(json, df1.schema)\n",
      " |      >>> df2.first()\n",
      " |      Row(field1=1, field2=u'row1', field3=Row(field4=11, field5=None), field6=None)\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import *\n",
      " |      >>> schema = StructType([\n",
      " |      ...     StructField(\"field2\", StringType()),\n",
      " |      ...     StructField(\"field3\",\n",
      " |      ...                 StructType([StructField(\"field5\", ArrayType(IntegerType()))]))\n",
      " |      ... ])\n",
      " |      >>> df3 = sqlContext.jsonRDD(json, schema)\n",
      " |      >>> df3.first()\n",
      " |      Row(field2=u'row1', field3=Row(field5=None))\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  load(self, path=None, source=None, schema=None, **options)\n",
      " |      Returns the dataset in a data source as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: Deprecated in 1.4, use :func:`DataFrameReader.load` instead.\n",
      " |  \n",
      " |  newSession(self)\n",
      " |      Returns a new SQLContext as new session, that has separate SQLConf,\n",
      " |      registered temporary tables and UDFs, but shared SparkContext and\n",
      " |      table cache.\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  parquetFile(self, *paths)\n",
      " |      Loads a Parquet file, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: Deprecated in 1.4, use :func:`DataFrameReader.parquet` instead.\n",
      " |      \n",
      " |      >>> sqlContext.parquetFile('python/test_support/sql/parquet_partitioned').dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numPartitions=None)\n",
      " |      Create a :class:`DataFrame` with single LongType column named `id`,\n",
      " |      containing elements in a range from `start` to `end` (exclusive) with\n",
      " |      step value `step`.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numPartitions: the number of partitions of the DataFrame\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.range(1, 7, 2).collect()\n",
      " |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      " |      \n",
      " |      If only one argument is specified, it will be used as the end value.\n",
      " |      \n",
      " |      >>> sqlContext.range(3).collect()\n",
      " |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  registerDataFrameAsTable(self, df, tableName)\n",
      " |      Registers the given :class:`DataFrame` as a temporary table in the catalog.\n",
      " |      \n",
      " |      Temporary tables exist only during the lifetime of this instance of :class:`SQLContext`.\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  registerFunction(self, name, f, returnType=StringType)\n",
      " |      Registers a python function (including lambda function) as a UDF\n",
      " |      so it can be used in SQL statements.\n",
      " |      \n",
      " |      In addition to a name and the function itself, the return type can be optionally specified.\n",
      " |      When the return type is not given it default to a string and conversion will automatically\n",
      " |      be done.  For any other return type, the produced object must match the specified type.\n",
      " |      \n",
      " |      :param name: name of the UDF\n",
      " |      :param f: python function\n",
      " |      :param returnType: a :class:`DataType` object\n",
      " |      \n",
      " |      >>> sqlContext.registerFunction(\"stringLengthString\", lambda x: len(x))\n",
      " |      >>> sqlContext.sql(\"SELECT stringLengthString('test')\").collect()\n",
      " |      [Row(_c0=u'4')]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import IntegerType\n",
      " |      >>> sqlContext.registerFunction(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      " |      >>> sqlContext.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      " |      [Row(_c0=4)]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import IntegerType\n",
      " |      >>> sqlContext.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      " |      >>> sqlContext.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      " |      [Row(_c0=4)]\n",
      " |      \n",
      " |      .. versionadded:: 1.2\n",
      " |  \n",
      " |  setConf(self, key, value)\n",
      " |      Sets the given Spark SQL configuration property.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sql(self, sqlQuery)\n",
      " |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> df2 = sqlContext.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
      " |      >>> df2.collect()\n",
      " |      [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  table(self, tableName)\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> df2 = sqlContext.table(\"table1\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  tableNames(self, dbName=None)\n",
      " |      Returns a list of names of tables in the database ``dbName``.\n",
      " |      \n",
      " |      :param dbName: string, name of the database to use. Default to the current database.\n",
      " |      :return: list of table names, in string\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> \"table1\" in sqlContext.tableNames()\n",
      " |      True\n",
      " |      >>> \"table1\" in sqlContext.tableNames(\"db\")\n",
      " |      True\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  tables(self, dbName=None)\n",
      " |      Returns a :class:`DataFrame` containing names of tables in the given database.\n",
      " |      \n",
      " |      If ``dbName`` is not specified, the current database will be used.\n",
      " |      \n",
      " |      The returned DataFrame has two columns: ``tableName`` and ``isTemporary``\n",
      " |      (a column with :class:`BooleanType` indicating if a table is a temporary one or not).\n",
      " |      \n",
      " |      :param dbName: string, name of the database to use.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> df2 = sqlContext.tables()\n",
      " |      >>> df2.filter(\"tableName = 'table1'\").first()\n",
      " |      Row(tableName=u'table1', isTemporary=True)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  uncacheTable(self, tableName)\n",
      " |      Removes the specified table from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getOrCreate(cls, sc) from __builtin__.type\n",
      " |      Get the existing SQLContext or create a new one with given SparkContext.\n",
      " |      \n",
      " |      :param sc: SparkContext\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  read\n",
      " |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      " |      in as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :return: :class:`DataFrameReader`\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  udf\n",
      " |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      " |      \n",
      " |      :return: :class:`UDFRegistration`\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use help to obtain more detailed information\n",
    "help(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.0'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After reading the help we've decided we want to use sc.version to see what version of Spark we are running\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed data and using a collection to create a DataFrame\n",
    "\n",
    "In Spark, datasets are represented as a list of entries, where the list is broken up into many different partitions that are each stored on a different machine.  Each partition holds a unique subset of the entries in the list.  Spark calls datasets that it stores \"Resilient Distributed Datasets\" (RDDs). Even DataFrames are ultimately represented as RDDs, with additional meta-data.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3b.png\" style=\"width: 900px; float: right; margin: 5px\"/>\n",
    "\n",
    "One of the defining features of Spark, compared to other data analytics frameworks (e.g., Hadoop), is that it stores data in memory rather than on disk.  This allows Spark applications to run much more quickly, because they are not slowed down by needing to read data from disk.\n",
    "The figure to the right illustrates how Spark breaks a list of data entries into partitions that are each stored in memory on a worker.\n",
    "\n",
    "\n",
    "To create the DataFrame, we'll use `sqlContext.createDataFrame()`, and we'll pass our array of data in as an argument to that function. Spark will create a new set of input data based on data that is passed in.  A DataFrame requires a _schema_, which is a list of columns, where each column has a name and a type. Our list of data has elements with types (mostly strings, but one integer). We'll supply the rest of the schema and the column names as the second argument to `createDataFrame()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with your first DataFrames\n",
    "\n",
    "In Spark, we first create a base [DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame). We can then apply one or more transformations to that base DataFrame. *A DataFrame is immutable, so once it is created, it cannot be changed.* As a result, each transformation creates a new DataFrame. Finally, we can apply one or more actions to the DataFrames.\n",
    "\n",
    "> Note that Spark uses lazy evaluation, so transformations are not actually executed until an action occurs.\n",
    "\n",
    "A DataFrame consists of a series of `Row` objects; each `Row` object has a set of named columns. You can think of a DataFrame as modeling a table, though the data source being processed does not have to be a table.\n",
    "\n",
    "More formally, a DataFrame must have a _schema_, which means it must consist of columns, each of which has a _name_ and a _type_. Some data sources have schemas built into them. Examples include RDBMS databases, Parquet files, and NoSQL databases like Cassandra. Other data sources don't have computer-readable schemas, but you can often apply a schema programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name=u'Bill', age=4)]\n"
     ]
    }
   ],
   "source": [
    "# Check that Spark is working\n",
    "from pyspark.sql import Row\n",
    "data = [('Alice', 1), ('Bob', 2), ('Bill', 4)]\n",
    "df = sqlContext.createDataFrame(data, ['name', 'age'])\n",
    "fil = df.filter(df.age > 3).collect()\n",
    "print fil\n",
    "\n",
    "# If the Spark job doesn't work properly this will raise an AssertionError\n",
    "assert fil == [Row(u'Bill', 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (2b) Loading a text file **\n",
    "\n",
    "Let's load a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124787\n"
     ]
    }
   ],
   "source": [
    "dataDF = sqlContext.read.text(\"file:/home/cloudera/Code/resources/Shakespeare.txt\")\n",
    "shakespeareCount = dataDF.count()\n",
    "\n",
    "print shakespeareCount\n",
    "\n",
    "# If the text file didn't load properly an AssertionError will be raised\n",
    "assert shakespeareCount == 124787"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use _collect_ to view results\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3d.png\" style=\"height:700px;float:right\"/>\n",
    "\n",
    "To see a list of elements decremented by one, we need to create a new list on the driver from the the data distributed in the executor nodes.  To do this we can call the `collect()` method on our DataFrame.  `collect()` is often used after transformations to ensure that we are only returning a *small* amount of data to the driver.  This is done because the data returned to the driver must fit into the driver's available memory.  If not, the driver will crash.\n",
    "\n",
    "The `collect()` method is the first action operation that we have encountered.  Action operations cause Spark to perform the (lazy) transformation operations that are required to compute the values returned by the action.  In our example, this means that tasks will now be launched to perform the `createDataFrame`, `select`, and `collect` operations.\n",
    "\n",
    "In the diagram, the dataset is broken into four partitions, so four `collect()` tasks are launched. Each task collects the entries in its partition and sends the result to the driver, which creates a list of the values, as shown in the figure below.\n",
    "\n",
    "A better way to visualize the data is to use the `show()` method. If you don't tell `show()` how many rows to display, it displays 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "| William Shakespeare|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "|almost no restric...|\n",
      "|re-use it under t...|\n",
      "|with this eBook o...|\n",
      "|                    |\n",
      "|** This is a COPY...|\n",
      "|**     Please fol...|\n",
      "|                    |\n",
      "|Title: The Comple...|\n",
      "|                    |\n",
      "|Author: William S...|\n",
      "|                    |\n",
      "|Posting Date: Sep...|\n",
      "|Release Date: Jan...|\n",
      "|                    |\n",
      "|   Language: English|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the schema in tree format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Alice|\n",
      "|  Bob|\n",
      "| Bill|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with dataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select everybody, but increment the age by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| name|(age + 1)|\n",
      "+-----+---------+\n",
      "|Alice|        2|\n",
      "|  Bob|        3|\n",
      "| Bill|        5|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name'], df['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select people older than 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use _count_ to get total\n",
    "\n",
    "One of the most basic jobs that we can run is the `count()` job which will count the number of elements in a DataFrame, using the `count()` action. Since `select()` creates a new DataFrame with the same number of elements as the starting DataFrame, we expect that applying `count()` to each DataFrame will return the same result.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3e.png\" style=\"height:700px;float:right\"/>\n",
    "\n",
    "Note that because `count()` is an action operation, if we had not already performed an action with `collect()`, then Spark would now perform the transformation operations when we executed `count()`.\n",
    "\n",
    "Each task counts the entries in its partition and sends the result to your SparkContext, which adds up all of the counts. The figure on the right shows what would happen if we ran `count()` on a small example dataset with just four partitions.\n",
    "\n",
    "Count people by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  2|    1|\n",
      "|  4|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "cs105_lab0",
  "notebookId": 2935288214771919
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
