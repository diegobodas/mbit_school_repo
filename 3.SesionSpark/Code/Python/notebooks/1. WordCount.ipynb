{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCount in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the canonical WordCount example, that is often used to start with MapReduce.\n",
    "\n",
    "We will read a large text file (downloaded from the Gutenberg project), parse it and do some counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the necessary modules and the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it we have the right context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7efbf4030890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our WordCount program. Remember:\n",
    "Map works by applying a function to each element in the list.\n",
    "flatMap works applying a function that returns a sequence for each element in the list, and flattering the results into the original list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    words = re.split(\"\\W+\", line)\n",
    "    return [(w.lower(), 1) for w in words if w]\n",
    "\n",
    "lines = sc.textFile(\"hdfs://localhost:8020/user/cloudera/books/Quijote.txt\")\n",
    "\n",
    "counts = lines.flatMap(mapper).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "output = counts.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naranja: 1\n",
      "escribe: 17\n",
      "escriba: 4\n",
      "deponer: 1\n",
      "escribo: 1\n",
      "escribi: 23\n",
      "otro: 511\n",
      "rompelle: 1\n",
      "avispas: 1\n"
     ]
    }
   ],
   "source": [
    "for (word, count) in output[1:10]:\n",
    "    print \"%s: %i\" % (word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare the output to be plotted: we want to show the distribution of word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'que', 20628),\n",
       " (u'de', 18217),\n",
       " (u'y', 18197),\n",
       " (u'a', 15627),\n",
       " (u'la', 10399),\n",
       " (u'en', 8251),\n",
       " (u'el', 8216),\n",
       " (u'se', 6970),\n",
       " (u'no', 6384),\n",
       " (u'los', 4755)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedOutput = sorted(output, key = lambda x: -x[1])[:10]\n",
    "sortedOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to build our plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEKCAYAAADXdbjqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN6cMUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18ca029590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [w[0] for w in sortedOutput]\n",
    "y_pos = range(len(sortedOutput))\n",
    "frequency = [w[1] for w in sortedOutput]\n",
    "\n",
    "plt.barh(y_pos, frequency[::-1], align='center', alpha=0.4)\n",
    "plt.yticks(y_pos, words[::-1])\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's stop the spark backend. VERY IMPORTANT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ejercicio.\n",
    "\n",
    "Repite la ejecución anterior pero:\n",
    "\n",
    "1. Elimina las palabras que aparecen menos de 50 veces.\n",
    "2. Elimina algunas stop-words (de, desde, a, ante, un, uno, una, unos...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posibles soluciones:\n",
    "\n",
    "1. counts = lines.flatMap(mapper).reduceByKey(lambda a, b: a+b).filter(lambda x: x[1] >= 50)\n",
    "2. Se plantea debajo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First define a list of uninteresting words, the stop words\n",
    "stopwords = frozenset(['a','de','uno','el','no','se'])\n",
    "\n",
    "# Broadcast this list to all worker processes in the cluster\n",
    "stopwords_bc = sc.broadcast(stopwords)\n",
    "\n",
    "# Create two accumulators for counting processed words\n",
    "stopword_count = sc.accumulator(0)\n",
    "regular_count = sc.accumulator(0)\n",
    "\n",
    "# Define a filter function\n",
    "def filter_word(w):\n",
    "    # Check if a given word is in the list of stopwords\n",
    "    if w in stopwords_bc.value:\n",
    "        stopword_count.add(1)\n",
    "        return False\n",
    "    else:\n",
    "        regular_count.add(1)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and process the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = sc.textFile('hdfs://localhost:8020/user/cloudera/books/Quijote.txt')\n",
    "words = text.flatMap(lambda line: line.split(\" \")) \\\n",
    "    .filter(filter_word) \\\n",
    "    .map(lambda word: (word,1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending = False) \n",
    "    #.map(lambda (k,v): k + ':' + str(v))\n",
    "# remember, HDFS does not overwrite with standard configuration\n",
    "# QuijoteSalida is a directory\n",
    "# words.saveAsTextFile('hdfs://localhost:8020/user/cloudera/books/QuijoteSalida')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from stackoverflow:\n",
    "\n",
    "saveAsTextFile is really processed by Spark executors. Depending on your Spark setup, Spark executors may run as a different user than your Spark application driver. I guess the spark application driver prepares the directory for the job fine, but then the executors running as a different user have no rights to write in that directory.\n",
    "\n",
    "Changing to 777 won't help, because permissions are not inherited by child dirs, so you'd get 755 anyways.\n",
    "\n",
    "Try running your Spark application as the same user that runs your Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o655.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30.0 (TID 76, quickstart.cloudera): java.io.IOException: Mkdirs failed to create file:/home/cloudera/Code/Python/notebooks/QuijoteSalida/_temporary/0/_temporary/attempt_201702151813_0030_m_000000_76 (exists=false, cwd=file:/yarn/nm/usercache/cloudera/appcache/application_1487176999350_0001/container_1487176999350_0001_01_000012)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:449)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:435)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:813)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\n\tat org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:91)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1193)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1185)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1843)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1856)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1933)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1213)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Mkdirs failed to create file:/home/cloudera/Code/Python/notebooks/QuijoteSalida/_temporary/0/_temporary/attempt_201702151813_0030_m_000000_76 (exists=false, cwd=file:/yarn/nm/usercache/cloudera/appcache/application_1487176999350_0001/container_1487176999350_0001_01_000012)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:449)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:435)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:813)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\n\tat org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:91)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1193)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1185)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a56a5b300449>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# save as local file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'file:/home/cloudera/Code/Python/notebooks/QuijoteSalida'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/lib/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1504\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1506\u001b[1;33m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1508\u001b[0m     \u001b[1;31m# Pair functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/lib/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/lib/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o655.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30.0 (TID 76, quickstart.cloudera): java.io.IOException: Mkdirs failed to create file:/home/cloudera/Code/Python/notebooks/QuijoteSalida/_temporary/0/_temporary/attempt_201702151813_0030_m_000000_76 (exists=false, cwd=file:/yarn/nm/usercache/cloudera/appcache/application_1487176999350_0001/container_1487176999350_0001_01_000012)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:449)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:435)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:813)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\n\tat org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:91)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1193)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1185)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1843)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1856)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1933)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1213)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Mkdirs failed to create file:/home/cloudera/Code/Python/notebooks/QuijoteSalida/_temporary/0/_temporary/attempt_201702151813_0030_m_000000_76 (exists=false, cwd=file:/yarn/nm/usercache/cloudera/appcache/application_1487176999350_0001/container_1487176999350_0001_01_000012)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:449)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:435)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:813)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\n\tat org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:91)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1193)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1185)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# save as local file\n",
    "words.saveAsTextFile('file:/home/cloudera/Code/Python/notebooks/QuijoteSalida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46111\n",
      "347653\n"
     ]
    }
   ],
   "source": [
    "# Print processing metrics\n",
    "print stopword_count.value\n",
    "print regular_count.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 15894\n",
      "la: 10200\n",
      ": 9504\n",
      "en: 7898\n",
      "los: 4680\n",
      "con: 4047\n",
      "por: 3758\n",
      "las: 3423\n",
      "lo: 3387\n"
     ]
    }
   ],
   "source": [
    "output = words.collect()\n",
    "for (word, count) in output[1:10]:\n",
    "    print \"%s: %i\" % (word, count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
